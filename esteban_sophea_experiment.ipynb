{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, os.path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as t\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLanguageNameDataset(Dataset):\n",
    "    \"\"\"Dataset of names of a single language; upon loading, all the names are lowercased\"\"\"\n",
    "\n",
    "    def __init__(self, name_dir: str, language: str = \"English\"):\n",
    "        \"\"\"\n",
    "        :param name_dir: the path to the directory containing the name files\n",
    "        :param language: the language of the names (with a capital letter at the begining)\n",
    "        \"\"\"\n",
    "        self.name_dir = name_dir\n",
    "        self.language = language\n",
    "        \n",
    "        self.language_file = os.path.join(self.name_dir, self.language + \".txt\")\n",
    "        self.language_file_length = None\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        # Lazy length loader\n",
    "        if self.language_file_length is None:\n",
    "            with open(self.language_file, 'r', encoding='utf8') as f:\n",
    "                # store the number of lines excluding empty lines at the end\n",
    "                self.language_file_length = len(f.read().rstrip().split('\\n'))\n",
    "        return self.language_file_length\n",
    "\n",
    "    def __getitem__(self, index: int) -> str:\n",
    "        assert index <= len(self), f\"Index '{index}' not found in file '{self.language_file}' of length {len(self)}\"\n",
    "\n",
    "        with open(self.language_file, 'r', encoding='utf8') as f:\n",
    "            # go through the file in a lazy way untill we get the target line\n",
    "            for line_number, line in enumerate(f):\n",
    "                if line_number == index:\n",
    "                    return self.language + '|' + line.strip().lower()\n",
    "        \n",
    "        # if we don't find the index even after going through the file, it means we have a huge anomaly\n",
    "        # combined with the assetr statement, this error should never be raised\n",
    "        raise IndexError(f\"Index '{index}' not found in file '{self.language_file}' of length {len(self)}\")\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__} object of {self.language} names; bound to file '{self.language_file}' of length {len(self)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLanguageNameDataset(ConcatDataset):\n",
    "    def __init__(self, name_dir: str, languages: t.Optional[t.Iterable[str]] = None):\n",
    "        \"\"\"\n",
    "        relies on multiple SingleLanguageNameDataset, one per possible language\n",
    "        :param name_dir: the path to the directory containing the name files,\n",
    "        :param languages: an iterable of the languages of the names (with a capital letter at the begining)\n",
    "            or None to take all the languages available\n",
    "        \"\"\"\n",
    "        # if the languages is None, we take all the languages available in the folder\n",
    "        if languages is None: languages = self.find_languages(name_dir)\n",
    "\n",
    "        self.languages = languages\n",
    "\n",
    "        # create one single language dataset per language, and use them to create the multi-language dataset\n",
    "        super().__init__(\n",
    "            [SingleLanguageNameDataset(name_dir, language) for language in self.languages]\n",
    "        )\n",
    "\n",
    "    def find_languages(self, directory: str, extension: str='.txt') -> t.Iterable[str]:\n",
    "        \"\"\"Finds all files with the extension, and use those files to create a list of available languages.\"\"\"\n",
    "        # gather all the files in the directory\n",
    "        files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "        languages = [language for language, ext in \n",
    "            [os.path.splitext(f) for f in files] # split into language name and extension\n",
    "             if ext == extension] # filter by extension\n",
    "        return languages\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__} object of a total of {len(self)} names from the following languages:\\n{self.languages}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"SingleLanguageNameDataset object of English names; bound to file 'corpus_names/names\\\\English.txt' of length 3668\",\n",
       " 'English|abraham',\n",
       " \"MultiLanguageNameDataset object of a total of 20074 names from the following languages:\\n['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\",\n",
       " 'Arabic|maalouf')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the class\n",
    "dataset = SingleLanguageNameDataset(\"corpus_names/names\", 'English')\n",
    "multi_dataset = MultiLanguageNameDataset(\"corpus_names/names\")\n",
    "str(dataset), dataset[5], str(multi_dataset), multi_dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameCollate(object):\n",
    "    \"\"\"Function object used as a collate function for DataLoader.\n",
    "    \n",
    "    Contains all the methods to build a chatacter-based encoding,\n",
    "    add a start symbol at the start and an end symbol at the end of every sample,\n",
    "    and apply padding and tensorisation to the batches\n",
    "    \"\"\" \n",
    "    \n",
    "    char_to_id: dict\n",
    "    id_to_char: t.List[int]\n",
    "    unk: str  # the replacement for unknown characters\n",
    "    start: str  # the start symbol\n",
    "    end: str  # the end symbol\n",
    "    pad: str  # the padding value\n",
    "\n",
    "    def __init__(self, unk: str=\"<UNK>\", start: str=\"<START>\", end: str=\"<END>\", pad: str=\" \"):\n",
    "        \"\"\"\n",
    "        :param unk: str, the replacement symbol for unknown characters\n",
    "        :param start: str, the start symbol\n",
    "        :param end: str, the end symbol\n",
    "        :param pad: str, the padding symbol\n",
    "        \"\"\"\n",
    "        self.unk, self.unk_id = unk, 0\n",
    "        self.start, self.start_id = start, 1\n",
    "        self.end, self.end_id = end, 2\n",
    "        self.pad, self.pad_id = pad, 3\n",
    "        self.first_unreserved_value = 4  # value to add to the character index to avoid overlap with reserved values\n",
    "        self.char_to_id = self.id_to_char = None # untrained character to index mapping\n",
    "\n",
    "    def _collate_fn(self, batch: t.Iterable[str]) -> torch.LongTensor:\n",
    "        # transform the input into numbers\n",
    "        batch = [self.char_to_tensor(sample) for sample in batch]\n",
    "        \n",
    "        # crop and add padding\n",
    "        batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=self.pad_id)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def __call__(self, batch: t.Iterable[str]) -> torch.LongTensor:\n",
    "        return self._collate_fn(batch)\n",
    "                \n",
    "    def char_to_tensor(self, string: str) -> torch.LongTensor:\n",
    "        # transforms the string itself\n",
    "        if self.char_to_id is None:\n",
    "            index_list = [ord(char) + self.first_unreserved_value for char in string]\n",
    "        else:\n",
    "            index_list = [self.char_to_id.get(char, self.unk_id - self.first_unreserved_value) + self.first_unreserved_value for char in string]\n",
    "\n",
    "        # add the start and end symbols\n",
    "        index_list = [self.start_id] + index_list + [self.end_id]\n",
    "        return torch.LongTensor(index_list)\n",
    "\n",
    "    def tensor_to_char(self, tensor: torch.LongTensor, keep_special_tokens: bool=True) -> str:\n",
    "        indexes = (int(index) for index in tensor.flatten()) # index generator\n",
    "        return ''.join(self.int_to_char(index, keep_special_tokens=keep_special_tokens) for index in indexes)\n",
    "        \n",
    "    def int_to_char(self, index: int, keep_special_tokens: bool=True) -> str:\n",
    "        # start and end symbol, padding symbol\n",
    "        if index == self.start_id: return self.start if keep_special_tokens else '' \n",
    "        elif index == self.end_id: return self.end if keep_special_tokens else '' \n",
    "        elif index == self.pad_id: return self.pad if keep_special_tokens else '' \n",
    "        \n",
    "        # transform either using the id_to_char mapping or the chr encoding\n",
    "        elif self.id_to_char is None: return chr(index - self.first_unreserved_value)\n",
    "        elif len(self.id_to_char) > index - self.first_unreserved_value >= 0: return self.id_to_char[int(index - self.first_unreserved_value)]\n",
    "        \n",
    "        # fallback on the unknown symbol\n",
    "        else: return self.unk\n",
    "\n",
    "    def train_char_mapping(self, dataset, max_chars: int=0) -> None:\n",
    "        \"\"\"Load all the characters in the data to build the character to integer mapping.\n",
    "        If max_chars is strictly higher than 0, the vocabulary will be limited to the max_chars most frequent characters.\n",
    "        If training is not done, ord() and chr() will be used to encode and decodde the characters.\n",
    "        \"\"\"\n",
    "        if max_chars > 0:\n",
    "            counter = Counter()\n",
    "            for sample in dataset:\n",
    "                counter.update(sample)\n",
    "\n",
    "            self.id_to_char = [char for char, count in counter.most_commonon(max_chars)]\n",
    "        else:\n",
    "            self.id_to_char = list(set(char for sample in dataset for char in sample))\n",
    "        self.char_to_id = {char: index for index, char in enumerate(self.id_to_char)}\n",
    "        \n",
    "    def get_voc_size(self) -> int:\n",
    "        \"\"\"Returns the size of the vocabulary, including the unkown, padding, start and stop tokens\"\"\"\n",
    "        # according to https://www.programiz.com/python-programming/methods/built-in/chr, chr() maps 1114112 characters\n",
    "        # that's why it's better to use a trained mapping in our case\n",
    "        \n",
    "        # size of the character mapping itself\n",
    "        mapping_size = 1114112 if self.id_to_char is None else len(self.id_to_char)\n",
    "        \n",
    "        return mapping_size + 4  # +1 for each of the unkown, padding, start and stop token, so +4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters & training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining default files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_folder = \"corpus_names/names\"\n",
    "dataset = MultiLanguageNameDataset(name_folder)\n",
    "#dataset = SingleLanguageNameDataset(name_folder, \"French\")\n",
    "# ratio of the dataset used for developement and evaluation, the remainder being allocated to training \n",
    "eval_ratio, dev_ratio = 0.1, 0.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, dev, eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x2b0033eda20>,\n",
       " <torch.utils.data.dataset.Subset at 0x2b0033eda58>,\n",
       " <torch.utils.data.dataset.Subset at 0x2b0033eda90>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of each subset\n",
    "eval_split = int(len(dataset) * eval_ratio)\n",
    "dev_split = int(len(dataset) * dev_ratio)\n",
    "train_split = len(dataset) - dev_split - eval_split\n",
    "\n",
    "# random split into each subset\n",
    "train_dataset, dev_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_split, dev_split, eval_split])\n",
    "train_dataset, dev_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 0#4\n",
    "\n",
    "# create the collate object and train the character mapping on the whole dataset\n",
    "noun_collate = NameCollate()\n",
    "noun_collate.train_char_mapping(dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=num_workers,\n",
    "                             collate_fn=noun_collate, pin_memory=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=num_workers,\n",
    "                             collate_fn=noun_collate, pin_memory=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=num_workers,\n",
    "                             collate_fn=noun_collate, pin_memory=True)\n",
    "\n",
    "dataset_loaders = {\"train\": train_dataloader, \"dev\": dev_dataloader, \"eval\": eval_dataloader}\n",
    "mode_method = {\"train\": lambda model: model.train(), \"dev\": lambda model: model.eval(), \"eval\": lambda model: model.eval()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the collate function and the data split for later use\n",
    "with open(\"dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            'full': dataset,\n",
    "            'train': train_dataset,\n",
    "            'dev': dev_dataset,\n",
    "            'eval': eval_dataset\n",
    "        }, f)\n",
    "with open(\"noun_collate.pkl\", \"wb\") as f:\n",
    "    pickle.dump(noun_collate, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 600\n",
    "log_step_batch = 200\n",
    "log_step_epoch = 10\n",
    "learning_rate = 1e-4\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=noun_collate.pad_id)  # CrossEntropyLoss ignoring padding\n",
    "optimizer_class = torch.optim.Adam#SGD\n",
    "\n",
    "checkpoint_file_path = \"model.state_dict.tch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = noun_collate.get_voc_size()  # size of the vocabulary, including unkown, padding, start and stop tokens\n",
    "embedding_size = 32\n",
    "hidden_size = 128 * 2\n",
    "rnn_layers = 2\n",
    "dropout_proba = 0.4#0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name generation network\n",
    "We use a character-based model to predict the next caracter of the name using a sequence to sequence approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size, rnn_layers=4, dropout_proba=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout_proba = dropout_proba\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        # embedding layer\n",
    "        self.emb = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.rnn_layers, dropout=self.dropout_proba, batch_first=True)\n",
    "        \n",
    "        # pre-output dropout layer\n",
    "        self.dropout_layer = nn.Dropout(self.dropout_proba)\n",
    "        \n",
    "        # output layer\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.vocabulary_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden = None):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # init hidden state\n",
    "        hidden = hidden if hidden is not None else self.init_hidden(x.size(0))\n",
    "        hidden = hidden.to(device)\n",
    "        \n",
    "        # embedding layer\n",
    "        out = self.emb(x)\n",
    "        \n",
    "        # RNN layers\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        \n",
    "        # pre-output dropout layer\n",
    "        out = self.dropout_layer(out)\n",
    "        \n",
    "        # output layer\n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initializes hidden state to 0\"\"\"\n",
    "        \n",
    "        return torch.zeros(self.rnn_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = NameRNN(vocabulary_size, embedding_size, hidden_size, rnn_layers, dropout_proba)\n",
    "    model = model.to(device)\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # training process\n",
    "    verbose = False\n",
    "\n",
    "    total_loss = {'train': [], 'dev': []}\n",
    "    best_dev_loss = 99999999\n",
    "    for epoch in range(epochs):\n",
    "        for phase in ['train', 'dev']:\n",
    "            mode_method[phase](model)\n",
    "            running_loss = []\n",
    "\n",
    "            for i, names in enumerate(dataset_loaders[phase]):\n",
    "                names = names.to(device)\n",
    "\n",
    "                # rnn forward\n",
    "                # as we are predicting the next character from the previous one,\n",
    "                # the input is all but the last character and the output is all but the first character\n",
    "                out, hidden = model(names[:, :-1])\n",
    "                # there is a need to flatten the batch dimension and the element dimension due to constraints on the loss\n",
    "                loss = criterion(out.reshape(-1, out.size(-1)), names[:, 1:].reshape(-1))\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # backward & opti\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                if verbose and (i + 1) % log_step_batch == 0:\n",
    "                    print(\"Epoch [{}/{}], \".format(epoch+1, epochs) +\n",
    "                          \"Phase {}, \".format(phase) +\n",
    "                          \"Batch [{}/{}], \".format(i+1, len(dataset_loaders[phase])) +\n",
    "                          \"Loss: {}\".format(loss.item()))\n",
    "\n",
    "            if (epoch + 1) % log_step_epoch == 0:\n",
    "                    print(\"Epoch [{}/{}], \".format(epoch+1, epochs) +\n",
    "                          \"Phase {}, \".format(phase) +\n",
    "                          \"Average loss: {}\".format(sum(running_loss)/len(running_loss)))\n",
    "\n",
    "            total_loss[phase].append(sum(running_loss)/len(running_loss))\n",
    "\n",
    "        # saving the model once per epoch, if the model is better than the previous checkpoint\n",
    "        if total_loss['dev'][-1] < best_dev_loss:\n",
    "            best_dev_loss = total_loss['dev'][-1]\n",
    "\n",
    "            torch.save(model.state_dict(), checkpoint_file_path)\n",
    "\n",
    "    ### Plot training and dev\n",
    "\n",
    "    best_model_epoch = total_loss['dev'].index(best_dev_loss)\n",
    "\n",
    "    plt.plot(total_loss['train'],  label='train')\n",
    "    plt.plot(total_loss['dev'],  label='dev', linestyle='dashed')\n",
    "    plt.axvline(x=best_model_epoch,color='gray',linestyle='dotted',  label='best dev loss')\n",
    "    plt.legend()\n",
    "    plt.plot()\n",
    "\n",
    "    print(\"best dev loss\", best_dev_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/600], Phase train, Average loss: 1.4130394042484344\n",
      "Epoch [10/600], Phase dev, Average loss: 1.3394445031881332\n",
      "Epoch [20/600], Phase train, Average loss: 1.2841349064357697\n",
      "Epoch [20/600], Phase dev, Average loss: 1.2237713187932968\n",
      "Epoch [30/600], Phase train, Average loss: 1.228424036313617\n",
      "Epoch [30/600], Phase dev, Average loss: 1.1762375310063362\n",
      "Epoch [40/600], Phase train, Average loss: 1.191821068998367\n",
      "Epoch [40/600], Phase dev, Average loss: 1.14321967959404\n",
      "Epoch [50/600], Phase train, Average loss: 1.1654690381080386\n",
      "Epoch [50/600], Phase dev, Average loss: 1.1198470443487167\n",
      "Epoch [60/600], Phase train, Average loss: 1.143792310404399\n",
      "Epoch [60/600], Phase dev, Average loss: 1.0999573394656181\n",
      "Epoch [70/600], Phase train, Average loss: 1.1257424345092168\n",
      "Epoch [70/600], Phase dev, Average loss: 1.0850710049271584\n",
      "Epoch [80/600], Phase train, Average loss: 1.1100131424646529\n",
      "Epoch [80/600], Phase dev, Average loss: 1.0738690793514252\n",
      "Epoch [90/600], Phase train, Average loss: 1.0974415275785658\n",
      "Epoch [90/600], Phase dev, Average loss: 1.0627570077776909\n",
      "Epoch [100/600], Phase train, Average loss: 1.0843435431283617\n",
      "Epoch [100/600], Phase dev, Average loss: 1.0534740015864372\n",
      "Epoch [110/600], Phase train, Average loss: 1.074122717456212\n",
      "Epoch [110/600], Phase dev, Average loss: 1.0460775047540665\n",
      "Epoch [120/600], Phase train, Average loss: 1.0625727025289384\n",
      "Epoch [120/600], Phase dev, Average loss: 1.0396431051194668\n",
      "Epoch [130/600], Phase train, Average loss: 1.0548847894819955\n",
      "Epoch [130/600], Phase dev, Average loss: 1.033189132809639\n",
      "Epoch [140/600], Phase train, Average loss: 1.0447913758338443\n",
      "Epoch [140/600], Phase dev, Average loss: 1.0305232219398022\n",
      "Epoch [150/600], Phase train, Average loss: 1.0359300885881697\n",
      "Epoch [150/600], Phase dev, Average loss: 1.028541211038828\n",
      "Epoch [160/600], Phase train, Average loss: 1.0291881206489744\n",
      "Epoch [160/600], Phase dev, Average loss: 1.0234410986304283\n",
      "Epoch [170/600], Phase train, Average loss: 1.022781042825608\n",
      "Epoch [170/600], Phase dev, Average loss: 1.0202665217220783\n",
      "Epoch [180/600], Phase train, Average loss: 1.0142244048534879\n",
      "Epoch [180/600], Phase dev, Average loss: 1.01856192573905\n",
      "Epoch [190/600], Phase train, Average loss: 1.0070563502727994\n",
      "Epoch [190/600], Phase dev, Average loss: 1.0161395967006683\n",
      "Epoch [200/600], Phase train, Average loss: 1.0013550642936948\n",
      "Epoch [200/600], Phase dev, Average loss: 1.0150102972984314\n",
      "Epoch [210/600], Phase train, Average loss: 0.99500282416268\n",
      "Epoch [210/600], Phase dev, Average loss: 1.0146243460476398\n",
      "Epoch [220/600], Phase train, Average loss: 0.9903408894463192\n",
      "Epoch [220/600], Phase dev, Average loss: 1.0147500857710838\n",
      "Epoch [230/600], Phase train, Average loss: 0.98410288398228\n",
      "Epoch [230/600], Phase dev, Average loss: 1.0135681554675102\n",
      "Epoch [240/600], Phase train, Average loss: 0.9793784230474442\n",
      "Epoch [240/600], Phase dev, Average loss: 1.0113705210387707\n",
      "Epoch [250/600], Phase train, Average loss: 0.9733895016094994\n",
      "Epoch [250/600], Phase dev, Average loss: 1.013625405728817\n",
      "Epoch [260/600], Phase train, Average loss: 0.967796179983351\n",
      "Epoch [260/600], Phase dev, Average loss: 1.0121636129915714\n",
      "Epoch [270/600], Phase train, Average loss: 0.9620335594056144\n",
      "Epoch [270/600], Phase dev, Average loss: 1.014381542801857\n",
      "Epoch [280/600], Phase train, Average loss: 0.9577260656016213\n",
      "Epoch [280/600], Phase dev, Average loss: 1.0128431655466557\n",
      "Epoch [290/600], Phase train, Average loss: 0.9525233526078482\n",
      "Epoch [290/600], Phase dev, Average loss: 1.0144594945013523\n",
      "Epoch [300/600], Phase train, Average loss: 0.9481904771592882\n",
      "Epoch [300/600], Phase dev, Average loss: 1.0158918127417564\n",
      "Epoch [310/600], Phase train, Average loss: 0.9442425076923673\n",
      "Epoch [310/600], Phase dev, Average loss: 1.016792856156826\n",
      "Epoch [320/600], Phase train, Average loss: 0.9395938908296918\n",
      "Epoch [320/600], Phase dev, Average loss: 1.0167872309684753\n",
      "Epoch [330/600], Phase train, Average loss: 0.9359448183150518\n",
      "Epoch [330/600], Phase dev, Average loss: 1.018474992364645\n",
      "Epoch [340/600], Phase train, Average loss: 0.9319625354948498\n",
      "Epoch [340/600], Phase dev, Average loss: 1.0194565765559673\n",
      "Epoch [350/600], Phase train, Average loss: 0.9259575955451481\n",
      "Epoch [350/600], Phase dev, Average loss: 1.0224234871566296\n",
      "Epoch [360/600], Phase train, Average loss: 0.9243350804798187\n",
      "Epoch [360/600], Phase dev, Average loss: 1.0226606726646423\n",
      "Epoch [370/600], Phase train, Average loss: 0.9208186420183333\n",
      "Epoch [370/600], Phase dev, Average loss: 1.0224290080368519\n",
      "Epoch [380/600], Phase train, Average loss: 0.9163538335807739\n",
      "Epoch [380/600], Phase dev, Average loss: 1.0258144102990627\n",
      "Epoch [390/600], Phase train, Average loss: 0.913256780968772\n",
      "Epoch [390/600], Phase dev, Average loss: 1.0276576429605484\n",
      "Epoch [400/600], Phase train, Average loss: 0.9110634497233799\n",
      "Epoch [400/600], Phase dev, Average loss: 1.0266206227242947\n",
      "Epoch [410/600], Phase train, Average loss: 0.9079480639525822\n",
      "Epoch [410/600], Phase dev, Average loss: 1.030068252235651\n",
      "Epoch [420/600], Phase train, Average loss: 0.9036832220024533\n",
      "Epoch [420/600], Phase dev, Average loss: 1.031402911990881\n",
      "Epoch [430/600], Phase train, Average loss: 0.8999643481913067\n",
      "Epoch [430/600], Phase dev, Average loss: 1.0327145606279373\n",
      "Epoch [440/600], Phase train, Average loss: 0.8975025628294263\n",
      "Epoch [440/600], Phase dev, Average loss: 1.0344679579138756\n",
      "Epoch [450/600], Phase train, Average loss: 0.8946360519954136\n",
      "Epoch [450/600], Phase dev, Average loss: 1.035866905003786\n",
      "Epoch [460/600], Phase train, Average loss: 0.8906724760456691\n",
      "Epoch [460/600], Phase dev, Average loss: 1.036520391702652\n",
      "Epoch [470/600], Phase train, Average loss: 0.8890472612683735\n",
      "Epoch [470/600], Phase dev, Average loss: 1.03976359218359\n",
      "Epoch [480/600], Phase train, Average loss: 0.8860660753552876\n",
      "Epoch [480/600], Phase dev, Average loss: 1.039268083870411\n",
      "Epoch [490/600], Phase train, Average loss: 0.8834719156462049\n",
      "Epoch [490/600], Phase dev, Average loss: 1.0437509343028069\n",
      "Epoch [500/600], Phase train, Average loss: 0.8815191179986984\n",
      "Epoch [500/600], Phase dev, Average loss: 1.0439709424972534\n",
      "Epoch [510/600], Phase train, Average loss: 0.8783841696050432\n",
      "Epoch [510/600], Phase dev, Average loss: 1.0480176471173763\n",
      "Epoch [520/600], Phase train, Average loss: 0.8800450601275005\n",
      "Epoch [520/600], Phase dev, Average loss: 1.0480411760509014\n",
      "Epoch [530/600], Phase train, Average loss: 0.875244876222005\n",
      "Epoch [530/600], Phase dev, Average loss: 1.047666773200035\n",
      "Epoch [540/600], Phase train, Average loss: 0.8711403319759975\n",
      "Epoch [540/600], Phase dev, Average loss: 1.0501324832439423\n",
      "Epoch [550/600], Phase train, Average loss: 0.8706224929718744\n",
      "Epoch [550/600], Phase dev, Average loss: 1.051037359982729\n",
      "Epoch [560/600], Phase train, Average loss: 0.8676720058161115\n",
      "Epoch [560/600], Phase dev, Average loss: 1.052683338522911\n",
      "Epoch [570/600], Phase train, Average loss: 0.8651583937425462\n",
      "Epoch [570/600], Phase dev, Average loss: 1.0545051097869873\n",
      "Epoch [580/600], Phase train, Average loss: 0.8616660558988177\n",
      "Epoch [580/600], Phase dev, Average loss: 1.0565998032689095\n",
      "Epoch [590/600], Phase train, Average loss: 0.8599930079210372\n",
      "Epoch [590/600], Phase dev, Average loss: 1.057330682873726\n",
      "Epoch [600/600], Phase train, Average loss: 0.8593674254795861\n",
      "Epoch [600/600], Phase dev, Average loss: 1.0589153245091438\n",
      "best dev loss 1.0112305171787739\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">\r\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\r\n",
       "  </style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 248.518125 \r\n",
       "L 372.103125 248.518125 \r\n",
       "L 372.103125 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 364.903125 224.64 \r\n",
       "L 364.903125 7.2 \r\n",
       "L 30.103125 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"m8a75d045d6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m8a75d045d6\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- 0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.133266\" xlink:href=\"#m8a75d045d6\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- 100 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(86.589516 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.945225\" xlink:href=\"#m8a75d045d6\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- 200 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(137.401475 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.757185\" xlink:href=\"#m8a75d045d6\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- 300 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 40.578125 39.3125 \r\n",
       "Q 47.65625 37.796875 51.625 33 \r\n",
       "Q 55.609375 28.21875 55.609375 21.1875 \r\n",
       "Q 55.609375 10.40625 48.1875 4.484375 \r\n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \r\n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \r\n",
       "Q 12.796875 0.390625 7.625 2.203125 \r\n",
       "L 7.625 11.71875 \r\n",
       "Q 11.71875 9.328125 16.59375 8.109375 \r\n",
       "Q 21.484375 6.890625 26.8125 6.890625 \r\n",
       "Q 36.078125 6.890625 40.9375 10.546875 \r\n",
       "Q 45.796875 14.203125 45.796875 21.1875 \r\n",
       "Q 45.796875 27.640625 41.28125 31.265625 \r\n",
       "Q 36.765625 34.90625 28.71875 34.90625 \r\n",
       "L 20.21875 34.90625 \r\n",
       "L 20.21875 43.015625 \r\n",
       "L 29.109375 43.015625 \r\n",
       "Q 36.375 43.015625 40.234375 45.921875 \r\n",
       "Q 44.09375 48.828125 44.09375 54.296875 \r\n",
       "Q 44.09375 59.90625 40.109375 62.90625 \r\n",
       "Q 36.140625 65.921875 28.71875 65.921875 \r\n",
       "Q 24.65625 65.921875 20.015625 65.03125 \r\n",
       "Q 15.375 64.15625 9.8125 62.3125 \r\n",
       "L 9.8125 71.09375 \r\n",
       "Q 15.4375 72.65625 20.34375 73.4375 \r\n",
       "Q 25.25 74.21875 29.59375 74.21875 \r\n",
       "Q 40.828125 74.21875 47.359375 69.109375 \r\n",
       "Q 53.90625 64.015625 53.90625 55.328125 \r\n",
       "Q 53.90625 49.265625 50.4375 45.09375 \r\n",
       "Q 46.96875 40.921875 40.578125 39.3125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-51\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(188.213435 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.569144\" xlink:href=\"#m8a75d045d6\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- 400 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 37.796875 64.3125 \r\n",
       "L 12.890625 25.390625 \r\n",
       "L 37.796875 25.390625 \r\n",
       "z\r\n",
       "M 35.203125 72.90625 \r\n",
       "L 47.609375 72.90625 \r\n",
       "L 47.609375 25.390625 \r\n",
       "L 58.015625 25.390625 \r\n",
       "L 58.015625 17.1875 \r\n",
       "L 47.609375 17.1875 \r\n",
       "L 47.609375 0 \r\n",
       "L 37.796875 0 \r\n",
       "L 37.796875 17.1875 \r\n",
       "L 4.890625 17.1875 \r\n",
       "L 4.890625 26.703125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-52\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(239.025394 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_6\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.381103\" xlink:href=\"#m8a75d045d6\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 500 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.796875 72.90625 \r\n",
       "L 49.515625 72.90625 \r\n",
       "L 49.515625 64.59375 \r\n",
       "L 19.828125 64.59375 \r\n",
       "L 19.828125 46.734375 \r\n",
       "Q 21.96875 47.46875 24.109375 47.828125 \r\n",
       "Q 26.265625 48.1875 28.421875 48.1875 \r\n",
       "Q 40.625 48.1875 47.75 41.5 \r\n",
       "Q 54.890625 34.8125 54.890625 23.390625 \r\n",
       "Q 54.890625 11.625 47.5625 5.09375 \r\n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \r\n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \r\n",
       "Q 12.796875 0.140625 7.71875 1.703125 \r\n",
       "L 7.71875 11.625 \r\n",
       "Q 12.109375 9.234375 16.796875 8.0625 \r\n",
       "Q 21.484375 6.890625 26.703125 6.890625 \r\n",
       "Q 35.15625 6.890625 40.078125 11.328125 \r\n",
       "Q 45.015625 15.765625 45.015625 23.390625 \r\n",
       "Q 45.015625 31 40.078125 35.4375 \r\n",
       "Q 35.15625 39.890625 26.703125 39.890625 \r\n",
       "Q 22.75 39.890625 18.8125 39.015625 \r\n",
       "Q 14.890625 38.140625 10.796875 36.28125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-53\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(289.837353 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_7\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"350.193063\" xlink:href=\"#m8a75d045d6\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 600 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(340.649313 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"m14f7bbf129\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m14f7bbf129\" y=\"204.213298\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 1.0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.6875 12.40625 \r\n",
       "L 21 12.40625 \r\n",
       "L 21 0 \r\n",
       "L 10.6875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-46\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 208.012517)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m14f7bbf129\" y=\"166.767527\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 1.5 -->\r\n",
       "      <g transform=\"translate(7.2 170.566746)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m14f7bbf129\" y=\"129.321757\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 2.0 -->\r\n",
       "      <g transform=\"translate(7.2 133.120976)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m14f7bbf129\" y=\"91.875987\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 2.5 -->\r\n",
       "      <g transform=\"translate(7.2 95.675205)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m14f7bbf129\" y=\"54.430216\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 3.0 -->\r\n",
       "      <g transform=\"translate(7.2 58.229435)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_13\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m14f7bbf129\" y=\"16.984446\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_13\">\r\n",
       "      <!-- 3.5 -->\r\n",
       "      <g transform=\"translate(7.2 20.783665)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_14\">\r\n",
       "    <path clip-path=\"url(#pb9820c45c7)\" d=\"M 45.321307 17.083636 \r\n",
       "L 45.829426 83.96593 \r\n",
       "L 46.337546 121.843816 \r\n",
       "L 46.845666 143.351812 \r\n",
       "L 47.353785 153.902246 \r\n",
       "L 47.861905 160.584245 \r\n",
       "L 48.370024 165.268264 \r\n",
       "L 48.878144 168.735943 \r\n",
       "L 49.386264 171.344402 \r\n",
       "L 50.402503 174.88627 \r\n",
       "L 51.418742 177.596426 \r\n",
       "L 52.434981 179.542454 \r\n",
       "L 52.943101 180.388342 \r\n",
       "L 56.499938 184.515141 \r\n",
       "L 57.516177 185.254738 \r\n",
       "L 58.532416 186.128231 \r\n",
       "L 60.564895 187.402758 \r\n",
       "L 61.073014 187.625104 \r\n",
       "L 62.597373 188.623434 \r\n",
       "L 68.186689 191.089424 \r\n",
       "L 69.202928 191.428871 \r\n",
       "L 73.267884 192.821052 \r\n",
       "L 73.776004 192.86423 \r\n",
       "L 77.840961 194.152227 \r\n",
       "L 78.8572 194.351984 \r\n",
       "L 80.889678 194.966839 \r\n",
       "L 82.414037 195.173963 \r\n",
       "L 85.462755 195.974244 \r\n",
       "L 85.970874 195.963813 \r\n",
       "L 88.003353 196.531249 \r\n",
       "L 89.527711 196.737968 \r\n",
       "L 92.068309 197.315356 \r\n",
       "L 93.084549 197.349162 \r\n",
       "L 94.100788 197.62725 \r\n",
       "L 94.608907 197.735792 \r\n",
       "L 95.117027 197.686499 \r\n",
       "L 96.133266 197.988365 \r\n",
       "L 96.641386 197.962385 \r\n",
       "L 97.657625 198.238232 \r\n",
       "L 98.165745 198.242942 \r\n",
       "L 98.673864 198.49711 \r\n",
       "L 99.690103 198.533639 \r\n",
       "L 100.706342 198.662133 \r\n",
       "L 109.344376 199.936166 \r\n",
       "L 109.852495 199.937347 \r\n",
       "L 111.376854 200.273142 \r\n",
       "L 111.884974 200.223423 \r\n",
       "L 112.393093 200.361399 \r\n",
       "L 112.901213 200.375551 \r\n",
       "L 113.409332 200.574629 \r\n",
       "L 113.917452 200.485551 \r\n",
       "L 114.933691 200.677151 \r\n",
       "L 115.441811 200.589812 \r\n",
       "L 115.94993 200.858803 \r\n",
       "L 116.45805 200.965584 \r\n",
       "L 116.966169 200.915988 \r\n",
       "L 117.474289 201.059615 \r\n",
       "L 119.506767 201.191913 \r\n",
       "L 120.014887 201.382219 \r\n",
       "L 122.555485 201.571862 \r\n",
       "L 123.063605 201.687786 \r\n",
       "L 123.571724 201.634462 \r\n",
       "L 124.587963 201.845031 \r\n",
       "L 125.604203 201.848619 \r\n",
       "L 126.112322 202.027355 \r\n",
       "L 127.128561 202.147793 \r\n",
       "L 128.144801 202.254336 \r\n",
       "L 131.193518 202.507191 \r\n",
       "L 132.209757 202.743808 \r\n",
       "L 133.225996 202.709935 \r\n",
       "L 133.734116 202.793541 \r\n",
       "L 134.242236 202.744397 \r\n",
       "L 135.258475 202.998543 \r\n",
       "L 142.880269 203.721572 \r\n",
       "L 143.388388 203.656257 \r\n",
       "L 144.404628 203.901913 \r\n",
       "L 145.420867 203.959059 \r\n",
       "L 146.945225 204.127598 \r\n",
       "L 147.453345 204.049149 \r\n",
       "L 147.961465 204.301733 \r\n",
       "L 148.469584 204.197113 \r\n",
       "L 149.993943 204.480994 \r\n",
       "L 151.010182 204.381416 \r\n",
       "L 151.518302 204.587544 \r\n",
       "L 153.042661 204.589 \r\n",
       "L 153.55078 204.785481 \r\n",
       "L 155.583259 204.821982 \r\n",
       "L 156.599498 204.936684 \r\n",
       "L 158.123857 205.052838 \r\n",
       "L 158.631976 205.230175 \r\n",
       "L 159.140096 205.163442 \r\n",
       "L 159.648215 205.283217 \r\n",
       "L 160.156335 205.209838 \r\n",
       "L 161.680694 205.403857 \r\n",
       "L 167.270009 205.837481 \r\n",
       "L 167.778129 205.953353 \r\n",
       "L 168.794368 205.928945 \r\n",
       "L 169.810607 206.209852 \r\n",
       "L 170.826846 206.04632 \r\n",
       "L 171.334966 206.241605 \r\n",
       "L 172.351205 206.256157 \r\n",
       "L 172.859325 206.325075 \r\n",
       "L 173.367444 206.240993 \r\n",
       "L 174.383684 206.39718 \r\n",
       "L 187.594793 207.385458 \r\n",
       "L 189.627271 207.48231 \r\n",
       "L 190.64351 207.602381 \r\n",
       "L 193.184108 207.867267 \r\n",
       "L 193.692228 207.972779 \r\n",
       "L 195.216587 207.838942 \r\n",
       "L 195.724706 207.861523 \r\n",
       "L 196.740946 208.039523 \r\n",
       "L 199.281544 208.263619 \r\n",
       "L 200.297783 208.223935 \r\n",
       "L 201.314022 208.425481 \r\n",
       "L 201.822142 208.340847 \r\n",
       "L 206.395218 208.78169 \r\n",
       "L 206.903337 208.691364 \r\n",
       "L 208.935816 208.76663 \r\n",
       "L 209.952055 208.8123 \r\n",
       "L 210.968294 209.07797 \r\n",
       "L 211.984533 208.955966 \r\n",
       "L 213.508892 209.07 \r\n",
       "L 214.525131 209.128034 \r\n",
       "L 215.033251 209.256017 \r\n",
       "L 216.04949 209.246268 \r\n",
       "L 216.55761 209.34795 \r\n",
       "L 217.065729 209.225959 \r\n",
       "L 218.590088 209.456716 \r\n",
       "L 219.606327 209.320685 \r\n",
       "L 220.114447 209.480871 \r\n",
       "L 221.130686 209.508393 \r\n",
       "L 222.146925 209.556809 \r\n",
       "L 222.655045 209.758448 \r\n",
       "L 223.163164 209.568221 \r\n",
       "L 224.687523 209.844901 \r\n",
       "L 225.195643 209.705588 \r\n",
       "L 225.703762 209.766691 \r\n",
       "L 226.211882 209.962434 \r\n",
       "L 226.720002 209.958925 \r\n",
       "L 227.228121 209.820299 \r\n",
       "L 229.768719 210.030175 \r\n",
       "L 231.293078 209.946756 \r\n",
       "L 231.801198 210.222622 \r\n",
       "L 232.817437 210.143312 \r\n",
       "L 233.833676 210.183228 \r\n",
       "L 234.341796 210.400072 \r\n",
       "L 234.849915 210.216784 \r\n",
       "L 236.374274 210.384091 \r\n",
       "L 236.882393 210.363865 \r\n",
       "L 237.390513 210.490893 \r\n",
       "L 239.422991 210.463521 \r\n",
       "L 240.439231 210.622178 \r\n",
       "L 240.94735 210.513487 \r\n",
       "L 242.979829 210.709631 \r\n",
       "L 243.487948 210.707166 \r\n",
       "L 243.996068 210.884698 \r\n",
       "L 244.504187 210.79924 \r\n",
       "L 246.028546 210.80153 \r\n",
       "L 248.061025 210.873893 \r\n",
       "L 249.585383 211.246493 \r\n",
       "L 250.093503 211.0004 \r\n",
       "L 251.109742 211.09588 \r\n",
       "L 252.125981 211.103858 \r\n",
       "L 254.666579 211.27632 \r\n",
       "L 257.715297 211.350608 \r\n",
       "L 258.731536 211.416041 \r\n",
       "L 261.272134 211.584358 \r\n",
       "L 261.780254 211.762021 \r\n",
       "L 262.288373 211.566111 \r\n",
       "L 263.812732 211.705202 \r\n",
       "L 264.320852 211.56081 \r\n",
       "L 266.861449 211.811524 \r\n",
       "L 267.369569 211.952308 \r\n",
       "L 267.877689 211.812326 \r\n",
       "L 268.385808 211.889489 \r\n",
       "L 269.402047 211.885984 \r\n",
       "L 274.991363 212.159275 \r\n",
       "L 275.499483 212.038405 \r\n",
       "L 276.007602 212.236905 \r\n",
       "L 284.645635 212.534223 \r\n",
       "L 286.678114 212.726906 \r\n",
       "L 288.202472 212.620082 \r\n",
       "L 288.710592 212.745985 \r\n",
       "L 289.218712 212.630514 \r\n",
       "L 289.726831 212.837295 \r\n",
       "L 291.25119 212.797573 \r\n",
       "L 291.75931 212.90182 \r\n",
       "L 292.267429 212.808662 \r\n",
       "L 292.775549 212.969316 \r\n",
       "L 294.299908 212.997169 \r\n",
       "L 302.937941 213.225861 \r\n",
       "L 303.44606 213.332506 \r\n",
       "L 304.970419 213.327183 \r\n",
       "L 305.478539 213.445168 \r\n",
       "L 306.494778 213.341175 \r\n",
       "L 308.019137 213.385057 \r\n",
       "L 308.527256 213.507781 \r\n",
       "L 309.035376 213.196908 \r\n",
       "L 309.543495 213.533694 \r\n",
       "L 310.051615 213.480963 \r\n",
       "L 311.067854 213.588049 \r\n",
       "L 311.575974 213.482912 \r\n",
       "L 312.592213 213.632931 \r\n",
       "L 314.116572 213.556401 \r\n",
       "L 314.624691 213.691821 \r\n",
       "L 316.14905 213.746723 \r\n",
       "L 317.165289 213.809197 \r\n",
       "L 317.673409 213.675276 \r\n",
       "L 318.181528 213.827411 \r\n",
       "L 318.689648 213.74678 \r\n",
       "L 319.197768 213.863797 \r\n",
       "L 319.705887 213.797596 \r\n",
       "L 320.214007 213.865868 \r\n",
       "L 320.722126 213.7665 \r\n",
       "L 321.230246 213.792111 \r\n",
       "L 321.738366 213.96792 \r\n",
       "L 322.754605 213.880905 \r\n",
       "L 323.262724 213.975022 \r\n",
       "L 324.278964 213.902579 \r\n",
       "L 325.295203 214.070972 \r\n",
       "L 330.884518 214.181666 \r\n",
       "L 333.425116 214.299168 \r\n",
       "L 334.949475 214.315146 \r\n",
       "L 335.965714 214.425112 \r\n",
       "L 336.981953 214.344075 \r\n",
       "L 337.490073 214.477434 \r\n",
       "L 337.998193 214.309367 \r\n",
       "L 339.522551 214.57334 \r\n",
       "L 341.04691 214.457052 \r\n",
       "L 342.063149 214.440877 \r\n",
       "L 343.079388 214.657084 \r\n",
       "L 344.095628 214.549992 \r\n",
       "L 344.603747 214.698637 \r\n",
       "L 345.111867 214.570138 \r\n",
       "L 346.128106 214.683064 \r\n",
       "L 346.636226 214.570009 \r\n",
       "L 347.652465 214.654234 \r\n",
       "L 348.160584 214.564578 \r\n",
       "L 349.176824 214.756364 \r\n",
       "L 349.684943 214.745488 \r\n",
       "L 349.684943 214.745488 \r\n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_15\">\r\n",
       "    <path clip-path=\"url(#pb9820c45c7)\" d=\"M 45.321307 67.706973 \r\n",
       "L 45.829426 109.208478 \r\n",
       "L 46.337546 140.544382 \r\n",
       "L 46.845666 154.696443 \r\n",
       "L 47.353785 162.740424 \r\n",
       "L 47.861905 168.186903 \r\n",
       "L 48.370024 172.12393 \r\n",
       "L 49.386264 177.115728 \r\n",
       "L 50.402503 180.525248 \r\n",
       "L 51.418742 182.728137 \r\n",
       "L 52.434981 184.469283 \r\n",
       "L 54.467459 186.981528 \r\n",
       "L 55.991818 188.41461 \r\n",
       "L 57.008057 189.11394 \r\n",
       "L 58.024297 189.902913 \r\n",
       "L 58.532416 190.035898 \r\n",
       "L 59.040536 190.514657 \r\n",
       "L 60.056775 191.014598 \r\n",
       "L 61.073014 191.681289 \r\n",
       "L 66.15421 193.952589 \r\n",
       "L 66.66233 194.052112 \r\n",
       "L 67.678569 194.518395 \r\n",
       "L 68.186689 194.634195 \r\n",
       "L 68.694808 194.924041 \r\n",
       "L 69.202928 194.949311 \r\n",
       "L 69.711047 195.1551 \r\n",
       "L 70.727286 195.396688 \r\n",
       "L 71.235406 195.706542 \r\n",
       "L 71.743526 195.741213 \r\n",
       "L 73.267884 196.373457 \r\n",
       "L 74.284124 196.467506 \r\n",
       "L 75.300363 196.727339 \r\n",
       "L 76.316602 197.018096 \r\n",
       "L 80.381559 197.842199 \r\n",
       "L 80.889678 198.042926 \r\n",
       "L 81.905918 198.124195 \r\n",
       "L 82.414037 198.348764 \r\n",
       "L 82.922157 198.338171 \r\n",
       "L 83.430276 198.459072 \r\n",
       "L 83.938396 198.441664 \r\n",
       "L 84.954635 198.692757 \r\n",
       "L 85.462755 198.681129 \r\n",
       "L 85.970874 198.941857 \r\n",
       "L 86.987113 199.030024 \r\n",
       "L 87.495233 199.222826 \r\n",
       "L 88.003353 199.11377 \r\n",
       "L 88.511472 199.269166 \r\n",
       "L 89.019592 199.301709 \r\n",
       "L 89.527711 199.578202 \r\n",
       "L 91.56019 199.716857 \r\n",
       "L 93.084549 199.975318 \r\n",
       "L 94.100788 199.899692 \r\n",
       "L 95.117027 200.245568 \r\n",
       "L 96.133266 200.196901 \r\n",
       "L 97.149505 200.338505 \r\n",
       "L 105.787538 201.244365 \r\n",
       "L 109.344376 201.41262 \r\n",
       "L 109.852495 201.580671 \r\n",
       "L 110.360615 201.545096 \r\n",
       "L 110.868734 201.727713 \r\n",
       "L 111.376854 201.764624 \r\n",
       "L 111.884974 201.528654 \r\n",
       "L 113.409332 201.737635 \r\n",
       "L 114.933691 201.858067 \r\n",
       "L 117.474289 202.008698 \r\n",
       "L 117.982409 202.12077 \r\n",
       "L 118.490528 202.002838 \r\n",
       "L 118.998648 202.164615 \r\n",
       "L 120.523007 202.040659 \r\n",
       "L 122.047365 202.210429 \r\n",
       "L 122.555485 202.311667 \r\n",
       "L 123.571724 202.1893 \r\n",
       "L 124.079844 202.36127 \r\n",
       "L 127.636681 202.423111 \r\n",
       "L 128.144801 202.553291 \r\n",
       "L 130.685398 202.571528 \r\n",
       "L 131.701638 202.755509 \r\n",
       "L 132.717877 202.565997 \r\n",
       "L 133.734116 202.780907 \r\n",
       "L 134.242236 202.59792 \r\n",
       "L 134.750355 202.769074 \r\n",
       "L 135.766594 202.622843 \r\n",
       "L 136.274714 202.823167 \r\n",
       "L 137.290953 202.783076 \r\n",
       "L 137.799073 202.921714 \r\n",
       "L 138.815312 202.836695 \r\n",
       "L 142.880269 203.022075 \r\n",
       "L 143.896508 202.924705 \r\n",
       "L 144.404628 203.01311 \r\n",
       "L 144.912747 202.987979 \r\n",
       "L 145.420867 202.837422 \r\n",
       "L 145.928986 202.871517 \r\n",
       "L 146.437106 203.089154 \r\n",
       "L 148.977704 202.97356 \r\n",
       "L 149.485823 203.238886 \r\n",
       "L 151.010182 203.092486 \r\n",
       "L 152.026421 203.121164 \r\n",
       "L 153.042661 202.955572 \r\n",
       "L 153.55078 203.243336 \r\n",
       "L 154.567019 203.096297 \r\n",
       "L 155.075139 203.223065 \r\n",
       "L 157.615737 203.140605 \r\n",
       "L 159.140096 203.1211 \r\n",
       "L 159.648215 203.080891 \r\n",
       "L 160.156335 203.234984 \r\n",
       "L 161.172574 203.227549 \r\n",
       "L 163.205052 203.163605 \r\n",
       "L 163.713172 203.293963 \r\n",
       "L 164.729411 203.148217 \r\n",
       "L 165.237531 203.179207 \r\n",
       "L 165.74565 203.050332 \r\n",
       "L 166.25377 203.322502 \r\n",
       "L 166.76189 203.361742 \r\n",
       "L 167.270009 203.170893 \r\n",
       "L 167.778129 203.303503 \r\n",
       "L 168.794368 203.186481 \r\n",
       "L 169.302488 203.140314 \r\n",
       "L 169.810607 203.254924 \r\n",
       "L 170.318727 203.096132 \r\n",
       "L 170.826846 203.171624 \r\n",
       "L 171.334966 203.112578 \r\n",
       "L 172.351205 203.295073 \r\n",
       "L 172.859325 203.196427 \r\n",
       "L 173.367444 203.372227 \r\n",
       "L 173.875564 203.21319 \r\n",
       "L 174.383684 203.34406 \r\n",
       "L 174.891803 203.193573 \r\n",
       "L 175.399923 203.356169 \r\n",
       "L 175.908042 203.232658 \r\n",
       "L 176.924281 203.302346 \r\n",
       "L 177.940521 203.236073 \r\n",
       "L 178.44864 203.156959 \r\n",
       "L 179.464879 203.255875 \r\n",
       "L 179.972999 203.316071 \r\n",
       "L 180.989238 203.035161 \r\n",
       "L 181.497358 203.298932 \r\n",
       "L 182.005477 203.136242 \r\n",
       "L 182.513597 203.30516 \r\n",
       "L 183.021717 203.152355 \r\n",
       "L 183.529836 203.293982 \r\n",
       "L 184.037956 203.107206 \r\n",
       "L 184.546075 203.103597 \r\n",
       "L 185.054195 203.263843 \r\n",
       "L 186.578554 203.2241 \r\n",
       "L 187.594793 203.200296 \r\n",
       "L 188.102913 203.17196 \r\n",
       "L 188.611032 203.023593 \r\n",
       "L 189.119152 203.18887 \r\n",
       "L 189.627271 203.106698 \r\n",
       "L 190.135391 203.310931 \r\n",
       "L 190.64351 203.104787 \r\n",
       "L 191.15163 203.1682 \r\n",
       "L 191.65975 203.049562 \r\n",
       "L 192.167869 203.130404 \r\n",
       "L 192.675989 203.054929 \r\n",
       "L 193.692228 203.062357 \r\n",
       "L 194.200348 202.932431 \r\n",
       "L 196.232826 203.12123 \r\n",
       "L 198.265304 203.136027 \r\n",
       "L 198.773424 202.795129 \r\n",
       "L 199.281544 203.086931 \r\n",
       "L 199.789663 202.887156 \r\n",
       "L 200.297783 203.067459 \r\n",
       "L 201.314022 203.01098 \r\n",
       "L 202.330261 202.955655 \r\n",
       "L 202.838381 202.919462 \r\n",
       "L 203.3465 203.031407 \r\n",
       "L 203.85462 202.961869 \r\n",
       "L 204.36274 203.065868 \r\n",
       "L 204.870859 202.9373 \r\n",
       "L 206.903337 203.05941 \r\n",
       "L 208.427696 202.825188 \r\n",
       "L 209.443935 202.815981 \r\n",
       "L 210.968294 202.957906 \r\n",
       "L 213.000773 202.770613 \r\n",
       "L 214.017012 202.80929 \r\n",
       "L 214.525131 202.989674 \r\n",
       "L 215.541371 202.668271 \r\n",
       "L 216.04949 202.835717 \r\n",
       "L 217.065729 202.636112 \r\n",
       "L 217.573849 202.756165 \r\n",
       "L 223.671284 202.590575 \r\n",
       "L 224.179404 202.680498 \r\n",
       "L 225.195643 202.560125 \r\n",
       "L 226.211882 202.590955 \r\n",
       "L 227.736241 202.516205 \r\n",
       "L 228.24436 202.574936 \r\n",
       "L 229.768719 202.321315 \r\n",
       "L 230.276839 202.367479 \r\n",
       "L 230.784958 202.54253 \r\n",
       "L 231.801198 202.369917 \r\n",
       "L 232.309317 202.560601 \r\n",
       "L 238.406752 202.236465 \r\n",
       "L 238.914872 202.355774 \r\n",
       "L 239.931111 202.182338 \r\n",
       "L 240.439231 202.313168 \r\n",
       "L 243.487948 202.035518 \r\n",
       "L 243.996068 202.243971 \r\n",
       "L 244.504187 202.141326 \r\n",
       "L 245.520427 202.322354 \r\n",
       "L 246.028546 202.161412 \r\n",
       "L 246.536666 202.248706 \r\n",
       "L 247.552905 202.124229 \r\n",
       "L 248.061025 202.219638 \r\n",
       "L 248.569144 202.027873 \r\n",
       "L 249.077264 202.143658 \r\n",
       "L 249.585383 201.950572 \r\n",
       "L 250.093503 201.925394 \r\n",
       "L 250.601622 202.030136 \r\n",
       "L 254.15846 201.990924 \r\n",
       "L 254.666579 202.079789 \r\n",
       "L 255.174699 201.890861 \r\n",
       "L 255.682818 201.937435 \r\n",
       "L 256.699058 201.808002 \r\n",
       "L 257.207177 201.946721 \r\n",
       "L 258.731536 201.774371 \r\n",
       "L 259.239656 201.879438 \r\n",
       "L 261.272134 201.731437 \r\n",
       "L 261.780254 201.900922 \r\n",
       "L 262.288373 201.698427 \r\n",
       "L 263.304612 201.763254 \r\n",
       "L 263.812732 201.661753 \r\n",
       "L 264.828971 201.863284 \r\n",
       "L 265.337091 201.56754 \r\n",
       "L 265.84521 201.742314 \r\n",
       "L 266.861449 201.871644 \r\n",
       "L 267.877689 201.533172 \r\n",
       "L 268.893928 201.711239 \r\n",
       "L 270.418287 201.562877 \r\n",
       "L 270.926406 201.657346 \r\n",
       "L 271.434526 201.364821 \r\n",
       "L 271.942645 201.760429 \r\n",
       "L 272.958885 201.541256 \r\n",
       "L 273.975124 201.60864 \r\n",
       "L 274.483243 201.376721 \r\n",
       "L 274.991363 201.465292 \r\n",
       "L 277.023841 201.285945 \r\n",
       "L 279.05632 201.573869 \r\n",
       "L 280.072559 201.347334 \r\n",
       "L 280.580678 201.445227 \r\n",
       "L 281.088798 201.391222 \r\n",
       "L 281.596918 201.176796 \r\n",
       "L 282.105037 201.422172 \r\n",
       "L 283.629396 201.235341 \r\n",
       "L 284.137516 201.274503 \r\n",
       "L 284.645635 201.513041 \r\n",
       "L 285.153755 201.011559 \r\n",
       "L 285.661874 201.255159 \r\n",
       "L 286.169994 201.070401 \r\n",
       "L 286.678114 201.258025 \r\n",
       "L 288.202472 201.079788 \r\n",
       "L 288.710592 201.272451 \r\n",
       "L 289.218712 201.125829 \r\n",
       "L 291.25119 201.007176 \r\n",
       "L 291.75931 200.984611 \r\n",
       "L 292.267429 201.159139 \r\n",
       "L 292.775549 200.84407 \r\n",
       "L 294.808027 201.022013 \r\n",
       "L 295.316147 200.796366 \r\n",
       "L 295.824266 201.01873 \r\n",
       "L 296.332386 201.112497 \r\n",
       "L 297.856745 200.903422 \r\n",
       "L 298.872984 200.920246 \r\n",
       "L 299.381103 200.740928 \r\n",
       "L 300.905462 200.820591 \r\n",
       "L 302.429821 200.647765 \r\n",
       "L 302.937941 200.79634 \r\n",
       "L 303.44606 200.788706 \r\n",
       "L 303.95418 200.617182 \r\n",
       "L 304.970419 200.702022 \r\n",
       "L 305.478539 200.972863 \r\n",
       "L 305.986658 200.695938 \r\n",
       "L 306.494778 200.659886 \r\n",
       "L 307.002897 200.77628 \r\n",
       "L 307.511017 200.645765 \r\n",
       "L 308.019137 200.82463 \r\n",
       "L 308.527256 200.870542 \r\n",
       "L 309.035376 200.61542 \r\n",
       "L 309.543495 200.747609 \r\n",
       "L 310.559735 200.531228 \r\n",
       "L 311.067854 200.680046 \r\n",
       "L 311.575974 200.538896 \r\n",
       "L 312.084093 200.236751 \r\n",
       "L 312.592213 200.690082 \r\n",
       "L 313.608452 200.601793 \r\n",
       "L 314.116572 200.64346 \r\n",
       "L 315.64093 200.40617 \r\n",
       "L 316.65717 200.57726 \r\n",
       "L 317.165289 200.451802 \r\n",
       "L 317.673409 200.567698 \r\n",
       "L 318.689648 200.387944 \r\n",
       "L 319.197768 200.458799 \r\n",
       "L 319.705887 200.346009 \r\n",
       "L 320.214007 200.41147 \r\n",
       "L 321.738366 200.297166 \r\n",
       "L 323.262724 200.382913 \r\n",
       "L 323.770844 200.255032 \r\n",
       "L 324.787083 200.511145 \r\n",
       "L 326.819561 200.175963 \r\n",
       "L 327.835801 200.443053 \r\n",
       "L 328.34392 200.148957 \r\n",
       "L 328.85204 200.350767 \r\n",
       "L 330.376399 200.12235 \r\n",
       "L 330.884518 200.310937 \r\n",
       "L 331.392638 200.032598 \r\n",
       "L 332.408877 200.232803 \r\n",
       "L 333.933236 199.955052 \r\n",
       "L 334.441355 200.131326 \r\n",
       "L 334.949475 199.978009 \r\n",
       "L 336.473834 199.988411 \r\n",
       "L 336.981953 200.123983 \r\n",
       "L 337.490073 199.929136 \r\n",
       "L 339.014432 199.855646 \r\n",
       "L 340.030671 200.112722 \r\n",
       "L 340.538791 199.931912 \r\n",
       "L 342.571269 200.027978 \r\n",
       "L 345.619986 199.816314 \r\n",
       "L 346.128106 199.87325 \r\n",
       "L 346.636226 199.759946 \r\n",
       "L 347.144345 199.877109 \r\n",
       "L 347.652465 199.760255 \r\n",
       "L 348.668704 199.789162 \r\n",
       "L 349.176824 199.722607 \r\n",
       "L 349.684943 199.801038 \r\n",
       "L 349.684943 199.801038 \r\n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_16\">\r\n",
       "    <path clip-path=\"url(#pb9820c45c7)\" d=\"M 173.367444 224.64 \r\n",
       "L 173.367444 7.2 \r\n",
       "\" style=\"fill:none;stroke:#808080;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 30.103125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 364.903125 224.64 \r\n",
       "L 364.903125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 364.903125 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 30.103125 7.2 \r\n",
       "L 364.903125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"legend_1\">\r\n",
       "    <g id=\"patch_7\">\r\n",
       "     <path d=\"M 260.179688 59.234375 \r\n",
       "L 357.903125 59.234375 \r\n",
       "Q 359.903125 59.234375 359.903125 57.234375 \r\n",
       "L 359.903125 14.2 \r\n",
       "Q 359.903125 12.2 357.903125 12.2 \r\n",
       "L 260.179688 12.2 \r\n",
       "Q 258.179688 12.2 258.179688 14.2 \r\n",
       "L 258.179688 57.234375 \r\n",
       "Q 258.179688 59.234375 260.179688 59.234375 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_17\">\r\n",
       "     <path d=\"M 262.179688 20.298437 \r\n",
       "L 282.179688 20.298437 \r\n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_18\"/>\r\n",
       "    <g id=\"text_14\">\r\n",
       "     <!-- train -->\r\n",
       "     <defs>\r\n",
       "      <path d=\"M 18.3125 70.21875 \r\n",
       "L 18.3125 54.6875 \r\n",
       "L 36.8125 54.6875 \r\n",
       "L 36.8125 47.703125 \r\n",
       "L 18.3125 47.703125 \r\n",
       "L 18.3125 18.015625 \r\n",
       "Q 18.3125 11.328125 20.140625 9.421875 \r\n",
       "Q 21.96875 7.515625 27.59375 7.515625 \r\n",
       "L 36.8125 7.515625 \r\n",
       "L 36.8125 0 \r\n",
       "L 27.59375 0 \r\n",
       "Q 17.1875 0 13.234375 3.875 \r\n",
       "Q 9.28125 7.765625 9.28125 18.015625 \r\n",
       "L 9.28125 47.703125 \r\n",
       "L 2.6875 47.703125 \r\n",
       "L 2.6875 54.6875 \r\n",
       "L 9.28125 54.6875 \r\n",
       "L 9.28125 70.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-116\"/>\r\n",
       "      <path d=\"M 41.109375 46.296875 \r\n",
       "Q 39.59375 47.171875 37.8125 47.578125 \r\n",
       "Q 36.03125 48 33.890625 48 \r\n",
       "Q 26.265625 48 22.1875 43.046875 \r\n",
       "Q 18.109375 38.09375 18.109375 28.8125 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 20.953125 51.171875 25.484375 53.578125 \r\n",
       "Q 30.03125 56 36.53125 56 \r\n",
       "Q 37.453125 56 38.578125 55.875 \r\n",
       "Q 39.703125 55.765625 41.0625 55.515625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-114\"/>\r\n",
       "      <path d=\"M 34.28125 27.484375 \r\n",
       "Q 23.390625 27.484375 19.1875 25 \r\n",
       "Q 14.984375 22.515625 14.984375 16.5 \r\n",
       "Q 14.984375 11.71875 18.140625 8.90625 \r\n",
       "Q 21.296875 6.109375 26.703125 6.109375 \r\n",
       "Q 34.1875 6.109375 38.703125 11.40625 \r\n",
       "Q 43.21875 16.703125 43.21875 25.484375 \r\n",
       "L 43.21875 27.484375 \r\n",
       "z\r\n",
       "M 52.203125 31.203125 \r\n",
       "L 52.203125 0 \r\n",
       "L 43.21875 0 \r\n",
       "L 43.21875 8.296875 \r\n",
       "Q 40.140625 3.328125 35.546875 0.953125 \r\n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \r\n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \r\n",
       "Q 6 8.015625 6 15.921875 \r\n",
       "Q 6 25.140625 12.171875 29.828125 \r\n",
       "Q 18.359375 34.515625 30.609375 34.515625 \r\n",
       "L 43.21875 34.515625 \r\n",
       "L 43.21875 35.40625 \r\n",
       "Q 43.21875 41.609375 39.140625 45 \r\n",
       "Q 35.0625 48.390625 27.6875 48.390625 \r\n",
       "Q 23 48.390625 18.546875 47.265625 \r\n",
       "Q 14.109375 46.140625 10.015625 43.890625 \r\n",
       "L 10.015625 52.203125 \r\n",
       "Q 14.9375 54.109375 19.578125 55.046875 \r\n",
       "Q 24.21875 56 28.609375 56 \r\n",
       "Q 40.484375 56 46.34375 49.84375 \r\n",
       "Q 52.203125 43.703125 52.203125 31.203125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-97\"/>\r\n",
       "      <path d=\"M 9.421875 54.6875 \r\n",
       "L 18.40625 54.6875 \r\n",
       "L 18.40625 0 \r\n",
       "L 9.421875 0 \r\n",
       "z\r\n",
       "M 9.421875 75.984375 \r\n",
       "L 18.40625 75.984375 \r\n",
       "L 18.40625 64.59375 \r\n",
       "L 9.421875 64.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-105\"/>\r\n",
       "      <path d=\"M 54.890625 33.015625 \r\n",
       "L 54.890625 0 \r\n",
       "L 45.90625 0 \r\n",
       "L 45.90625 32.71875 \r\n",
       "Q 45.90625 40.484375 42.875 44.328125 \r\n",
       "Q 39.84375 48.1875 33.796875 48.1875 \r\n",
       "Q 26.515625 48.1875 22.3125 43.546875 \r\n",
       "Q 18.109375 38.921875 18.109375 30.90625 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 21.34375 51.125 25.703125 53.5625 \r\n",
       "Q 30.078125 56 35.796875 56 \r\n",
       "Q 45.21875 56 50.046875 50.171875 \r\n",
       "Q 54.890625 44.34375 54.890625 33.015625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-110\"/>\r\n",
       "     </defs>\r\n",
       "     <g transform=\"translate(290.179688 23.798437)scale(0.1 -0.1)\">\r\n",
       "      <use xlink:href=\"#DejaVuSans-116\"/>\r\n",
       "      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_19\">\r\n",
       "     <path d=\"M 262.179688 34.976562 \r\n",
       "L 282.179688 34.976562 \r\n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_20\"/>\r\n",
       "    <g id=\"text_15\">\r\n",
       "     <!-- dev -->\r\n",
       "     <defs>\r\n",
       "      <path d=\"M 45.40625 46.390625 \r\n",
       "L 45.40625 75.984375 \r\n",
       "L 54.390625 75.984375 \r\n",
       "L 54.390625 0 \r\n",
       "L 45.40625 0 \r\n",
       "L 45.40625 8.203125 \r\n",
       "Q 42.578125 3.328125 38.25 0.953125 \r\n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \r\n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \r\n",
       "Q 5.515625 14.40625 5.515625 27.296875 \r\n",
       "Q 5.515625 40.1875 11.734375 48.09375 \r\n",
       "Q 17.96875 56 27.875 56 \r\n",
       "Q 33.9375 56 38.25 53.625 \r\n",
       "Q 42.578125 51.265625 45.40625 46.390625 \r\n",
       "z\r\n",
       "M 14.796875 27.296875 \r\n",
       "Q 14.796875 17.390625 18.875 11.75 \r\n",
       "Q 22.953125 6.109375 30.078125 6.109375 \r\n",
       "Q 37.203125 6.109375 41.296875 11.75 \r\n",
       "Q 45.40625 17.390625 45.40625 27.296875 \r\n",
       "Q 45.40625 37.203125 41.296875 42.84375 \r\n",
       "Q 37.203125 48.484375 30.078125 48.484375 \r\n",
       "Q 22.953125 48.484375 18.875 42.84375 \r\n",
       "Q 14.796875 37.203125 14.796875 27.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-100\"/>\r\n",
       "      <path d=\"M 56.203125 29.59375 \r\n",
       "L 56.203125 25.203125 \r\n",
       "L 14.890625 25.203125 \r\n",
       "Q 15.484375 15.921875 20.484375 11.0625 \r\n",
       "Q 25.484375 6.203125 34.421875 6.203125 \r\n",
       "Q 39.59375 6.203125 44.453125 7.46875 \r\n",
       "Q 49.3125 8.734375 54.109375 11.28125 \r\n",
       "L 54.109375 2.78125 \r\n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \r\n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \r\n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \r\n",
       "Q 5.515625 13.8125 5.515625 26.8125 \r\n",
       "Q 5.515625 40.234375 12.765625 48.109375 \r\n",
       "Q 20.015625 56 32.328125 56 \r\n",
       "Q 43.359375 56 49.78125 48.890625 \r\n",
       "Q 56.203125 41.796875 56.203125 29.59375 \r\n",
       "z\r\n",
       "M 47.21875 32.234375 \r\n",
       "Q 47.125 39.59375 43.09375 43.984375 \r\n",
       "Q 39.0625 48.390625 32.421875 48.390625 \r\n",
       "Q 24.90625 48.390625 20.390625 44.140625 \r\n",
       "Q 15.875 39.890625 15.1875 32.171875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-101\"/>\r\n",
       "      <path d=\"M 2.984375 54.6875 \r\n",
       "L 12.5 54.6875 \r\n",
       "L 29.59375 8.796875 \r\n",
       "L 46.6875 54.6875 \r\n",
       "L 56.203125 54.6875 \r\n",
       "L 35.6875 0 \r\n",
       "L 23.484375 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-118\"/>\r\n",
       "     </defs>\r\n",
       "     <g transform=\"translate(290.179688 38.476562)scale(0.1 -0.1)\">\r\n",
       "      <use xlink:href=\"#DejaVuSans-100\"/>\r\n",
       "      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "      <use x=\"125\" xlink:href=\"#DejaVuSans-118\"/>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_21\">\r\n",
       "     <path d=\"M 262.179688 49.654687 \r\n",
       "L 282.179688 49.654687 \r\n",
       "\" style=\"fill:none;stroke:#808080;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_22\"/>\r\n",
       "    <g id=\"text_16\">\r\n",
       "     <!-- best dev loss -->\r\n",
       "     <defs>\r\n",
       "      <path d=\"M 48.6875 27.296875 \r\n",
       "Q 48.6875 37.203125 44.609375 42.84375 \r\n",
       "Q 40.53125 48.484375 33.40625 48.484375 \r\n",
       "Q 26.265625 48.484375 22.1875 42.84375 \r\n",
       "Q 18.109375 37.203125 18.109375 27.296875 \r\n",
       "Q 18.109375 17.390625 22.1875 11.75 \r\n",
       "Q 26.265625 6.109375 33.40625 6.109375 \r\n",
       "Q 40.53125 6.109375 44.609375 11.75 \r\n",
       "Q 48.6875 17.390625 48.6875 27.296875 \r\n",
       "z\r\n",
       "M 18.109375 46.390625 \r\n",
       "Q 20.953125 51.265625 25.265625 53.625 \r\n",
       "Q 29.59375 56 35.59375 56 \r\n",
       "Q 45.5625 56 51.78125 48.09375 \r\n",
       "Q 58.015625 40.1875 58.015625 27.296875 \r\n",
       "Q 58.015625 14.40625 51.78125 6.484375 \r\n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \r\n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \r\n",
       "Q 20.953125 3.328125 18.109375 8.203125 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 75.984375 \r\n",
       "L 18.109375 75.984375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-98\"/>\r\n",
       "      <path d=\"M 44.28125 53.078125 \r\n",
       "L 44.28125 44.578125 \r\n",
       "Q 40.484375 46.53125 36.375 47.5 \r\n",
       "Q 32.28125 48.484375 27.875 48.484375 \r\n",
       "Q 21.1875 48.484375 17.84375 46.4375 \r\n",
       "Q 14.5 44.390625 14.5 40.28125 \r\n",
       "Q 14.5 37.15625 16.890625 35.375 \r\n",
       "Q 19.28125 33.59375 26.515625 31.984375 \r\n",
       "L 29.59375 31.296875 \r\n",
       "Q 39.15625 29.25 43.1875 25.515625 \r\n",
       "Q 47.21875 21.78125 47.21875 15.09375 \r\n",
       "Q 47.21875 7.46875 41.1875 3.015625 \r\n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \r\n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \r\n",
       "Q 10.6875 0.296875 5.421875 2 \r\n",
       "L 5.421875 11.28125 \r\n",
       "Q 10.40625 8.6875 15.234375 7.390625 \r\n",
       "Q 20.0625 6.109375 24.8125 6.109375 \r\n",
       "Q 31.15625 6.109375 34.5625 8.28125 \r\n",
       "Q 37.984375 10.453125 37.984375 14.40625 \r\n",
       "Q 37.984375 18.0625 35.515625 20.015625 \r\n",
       "Q 33.0625 21.96875 24.703125 23.78125 \r\n",
       "L 21.578125 24.515625 \r\n",
       "Q 13.234375 26.265625 9.515625 29.90625 \r\n",
       "Q 5.8125 33.546875 5.8125 39.890625 \r\n",
       "Q 5.8125 47.609375 11.28125 51.796875 \r\n",
       "Q 16.75 56 26.8125 56 \r\n",
       "Q 31.78125 56 36.171875 55.265625 \r\n",
       "Q 40.578125 54.546875 44.28125 53.078125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-115\"/>\r\n",
       "      <path id=\"DejaVuSans-32\"/>\r\n",
       "      <path d=\"M 9.421875 75.984375 \r\n",
       "L 18.40625 75.984375 \r\n",
       "L 18.40625 0 \r\n",
       "L 9.421875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-108\"/>\r\n",
       "      <path d=\"M 30.609375 48.390625 \r\n",
       "Q 23.390625 48.390625 19.1875 42.75 \r\n",
       "Q 14.984375 37.109375 14.984375 27.296875 \r\n",
       "Q 14.984375 17.484375 19.15625 11.84375 \r\n",
       "Q 23.34375 6.203125 30.609375 6.203125 \r\n",
       "Q 37.796875 6.203125 41.984375 11.859375 \r\n",
       "Q 46.1875 17.53125 46.1875 27.296875 \r\n",
       "Q 46.1875 37.015625 41.984375 42.703125 \r\n",
       "Q 37.796875 48.390625 30.609375 48.390625 \r\n",
       "z\r\n",
       "M 30.609375 56 \r\n",
       "Q 42.328125 56 49.015625 48.375 \r\n",
       "Q 55.71875 40.765625 55.71875 27.296875 \r\n",
       "Q 55.71875 13.875 49.015625 6.21875 \r\n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \r\n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \r\n",
       "Q 5.515625 13.875 5.515625 27.296875 \r\n",
       "Q 5.515625 40.765625 12.171875 48.375 \r\n",
       "Q 18.84375 56 30.609375 56 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-111\"/>\r\n",
       "     </defs>\r\n",
       "     <g transform=\"translate(290.179688 53.154687)scale(0.1 -0.1)\">\r\n",
       "      <use xlink:href=\"#DejaVuSans-98\"/>\r\n",
       "      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "      <use x=\"125\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "      <use x=\"177.099609\" xlink:href=\"#DejaVuSans-116\"/>\r\n",
       "      <use x=\"216.308594\" xlink:href=\"#DejaVuSans-32\"/>\r\n",
       "      <use x=\"248.095703\" xlink:href=\"#DejaVuSans-100\"/>\r\n",
       "      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "      <use x=\"373.095703\" xlink:href=\"#DejaVuSans-118\"/>\r\n",
       "      <use x=\"432.275391\" xlink:href=\"#DejaVuSans-32\"/>\r\n",
       "      <use x=\"464.0625\" xlink:href=\"#DejaVuSans-108\"/>\r\n",
       "      <use x=\"491.845703\" xlink:href=\"#DejaVuSans-111\"/>\r\n",
       "      <use x=\"553.027344\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "      <use x=\"605.126953\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"pb9820c45c7\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & metrics\n",
    "- Run the model on the eval set\n",
    "- Evaluate the model using perplexity, cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the model and data if necessary\n",
    "model_reloaded = NameRNN(vocabulary_size, embedding_size, hidden_size, rnn_layers, dropout_proba)\n",
    "model_reloaded.load_state_dict(torch.load(checkpoint_file_path, map_location='cpu'))\n",
    "model_reloaded_gpu = model_reloaded.to(device)\n",
    "\n",
    "# save the collate function and the data split for later use\n",
    "with open(\"dataset.pkl\", \"rb\") as f:\n",
    "    dataset_dict = pickle.load(f)\n",
    "    \n",
    "    # unpack the objects\n",
    "    dataset = dataset_dict['full']\n",
    "    train_dataset = dataset_dict['train']\n",
    "    dev_dataset = dataset_dict['dev']\n",
    "    eval_dataset = dataset_dict['eval']\n",
    "    \n",
    "    # recreate the dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=num_workers,\n",
    "                                 collate_fn=noun_collate, pin_memory=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=num_workers,\n",
    "                                 collate_fn=noun_collate, pin_memory=True)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=num_workers,\n",
    "                                 collate_fn=noun_collate, pin_memory=True)\n",
    "    dataset_loaders = {\"train\": train_dataloader, \"dev\": dev_dataloader, \"eval\": eval_dataloader}\n",
    "        \n",
    "with open(\"noun_collate.pkl\", \"rb\") as f:\n",
    "    noun_collate = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross entropy: 1.086506575345993\n",
      "Average perplexity: 2.963901798118446\n"
     ]
    }
   ],
   "source": [
    "total_loss = []\n",
    "results = []\n",
    "for i, names in enumerate(dataset_loaders['dev']):\n",
    "    names = names.to(device)\n",
    "\n",
    "    # autoencoder forward\n",
    "    # as we are predicting the next character from the previous one,\n",
    "    # the input is all but the last character and the output is all but the first character\n",
    "    out, hidden = model_reloaded_gpu(names[:, :-1])\n",
    "    loss = criterion(out.reshape(-1, out.size(-1)), names[:, 1:].reshape(-1))\n",
    "    results.append(torch.max(out, -1)[1])\n",
    "\n",
    "    total_loss.append(loss.cpu().item())\n",
    "\n",
    "# for perplexity, we just need to apply the exponential on the cross entropy loss to obtain it, according to \n",
    "# https://stackoverflow.com/questions/59209086/calculate-perplexity-in-pytorch\n",
    "perplexity = math.exp(sum(total_loss)/len(total_loss))\n",
    "print(\"Average cross entropy: {}\".format(sum(total_loss)/len(total_loss)))\n",
    "print(\"Average perplexity: {}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results generated for the eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rutch|aaiuwessengaknnk\n",
      "Rutch|aaiuwessengaknnk\n"
     ]
    }
   ],
   "source": [
    "evaluation_result_file = \"evaluation_outputs\"\n",
    "#torch.save(results, evaluation_result_file.cpu())\n",
    "print(noun_collate.tensor_to_char(results[0][0], keep_special_tokens=False))\n",
    "print(noun_collate.tensor_to_char(results[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(noun_collate, model):\n",
    "    last_character = torch.LongTensor([[noun_collate.start_id]]).to(device)\n",
    "    string = \"\"\n",
    "    hidden = model.init_hidden(1).to(device)\n",
    "    while last_character.item() != noun_collate.end_id and len(string) < 30:\n",
    "        out, hidden = model.to(device)(last_character.to(device), hidden.to(device))\n",
    "\n",
    "        last_character = torch.multinomial(torch.softmax(out[0].cpu(), -1), 1)\n",
    "        last_character_string = noun_collate.tensor_to_char(last_character)\n",
    "        string += last_character_string\n",
    "\n",
    "        #print(\"new char:\", last_character, last_character_string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Czecc|kouly<END>',\n",
       " 'Russian|janziev<END>',\n",
       " 'Arabic|salib<END>',\n",
       " 'Russian|toaly<END>',\n",
       " 'Russian|izerenko<END>',\n",
       " 'Russian|bakzhin<END>',\n",
       " 'Russian|chuvin<END>',\n",
       " 'Arabic|toma<END>',\n",
       " 'Russian|jortovski<END>',\n",
       " 'Russian|danyuchenko<END>',\n",
       " 'Czech|jovandin<END>',\n",
       " 'English|jutt<END>',\n",
       " 'Russian|katdenov<END>',\n",
       " 'Russian|martynaev<END>',\n",
       " 'Russian|jadydkins<END>',\n",
       " 'Italian|totelli<END>',\n",
       " 'English|fair<END>',\n",
       " 'Arabic|kathani<END>',\n",
       " 'Scotdish|lacklidd<END>',\n",
       " 'Dutch|schromendege<END>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[generate(noun_collate, model_reloaded_gpu) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>ab<END>\n"
     ]
    }
   ],
   "source": [
    "print(noun_collate.tensor_to_char(noun_collate.char_to_tensor(\"ab\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook esteban_sophea_experiment.ipynb to html\n",
      "[NbConvertApp] Writing 412104 bytes to esteban_sophea_experiment.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert esteban_sophea_experiment.ipynb --to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
